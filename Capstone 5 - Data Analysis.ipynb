{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>Movement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ladies gentleman stand begin good welcome delt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>stand begin good lady gentleman welcome jp mor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>hello welcome earning review chief executive o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>welcome bank america earnings announcement tim...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>good conference operator time like welcome wel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  Movement\n",
       "0  ladies gentleman stand begin good welcome delt...         1\n",
       "1  stand begin good lady gentleman welcome jp mor...         1\n",
       "2  hello welcome earning review chief executive o...         1\n",
       "3  welcome bank america earnings announcement tim...         2\n",
       "4  good conference operator time like welcome wel...         1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! Python3\n",
    "# by Jacob Kovach\n",
    "# Confidential and Proprietary\n",
    "\n",
    "import numpy as np, pandas as pd, urllib.request, pickle, spacy, re, os\n",
    "from nltk import tokenize\n",
    "from collections import Counter\n",
    "\n",
    "with open('/Users/jkovach/Downloads/earnings-call-transcripts/_call_df_500', 'rb') as file:\n",
    "    earnings_df = pickle.load(file)\n",
    "earnings_df = earnings_df.drop(['datetime', 'filename', 'ticker', 'raw', 'header_check', \n",
    "                                'footer_check', 'price_delta', 'duration'], 1)\n",
    "print(earnings_df.shape)\n",
    "earnings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#splitting into training and test sets\\nX_train_tfidf, X_test_tfidf= train_test_split(earnings_tfidf, test_size=sample_split, random_state=0)\\n\\n#Reduce the feature space from 1379 to 225.\\nsvd = TruncatedSVD(250)\\nlsa = make_pipeline(svd, Normalizer(copy=False))\\n# Run SVD on the training data, then project the training data.\\nX_train_lsa = lsa.fit_transform(X_train_tfidf)\\n\\nvariance_explained=svd.explained_variance_ratio_\\ntotal_variance = variance_explained.sum()\\nprint(\"Percent variance captured by all components:\", total_variance*100)\\n\\n#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\\nparas_by_component=pd.DataFrame(X_train_lsa,index=X_train)\\nparas_by_component=paras_by_component.merge(earnings_df, on=\\'content\\')\\nparas_by_component.head()'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "sample_split=0.3\n",
    "X_train, X_test = train_test_split(earnings_df['content'], test_size=sample_split, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.4, \n",
    "                             min_df=0.1, \n",
    "                             stop_words='english', \n",
    "                             lowercase=True, \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2', \n",
    "                             smooth_idf=True,\n",
    "                             ngram_range=(1,3),\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "earnings_tfidf = vectorizer.fit_transform(earnings_df['content']).toarray()\n",
    "labels = earnings_df['Movement']\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Number of features: %d\" % len(earnings_tfidf))\n",
    "\n",
    "\"\"\"#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(earnings_tfidf, test_size=sample_split, random_state=0)\n",
    "\n",
    "#Reduce the feature space from 1379 to 225.\n",
    "svd = TruncatedSVD(250)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\", total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "paras_by_component=paras_by_component.merge(earnings_df, on='content')\n",
    "paras_by_component.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Balance: [(0, 106), (1, 227), (2, 92)]\n",
      "Class Balance: [(0, 158), (1, 158), (2, 158)]\n",
      "Class Balance: [(0, 31), (1, 69), (2, 28)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "sampler_state = 42\n",
    "ros = RandomOverSampler(random_state=sampler_state)\n",
    "rus = RandomUnderSampler(random_state=sampler_state)\n",
    "sm = SMOTE(random_state=sampler_state)\n",
    "\n",
    "\"\"\"y = paras_by_component['Movement']\n",
    "X = paras_by_component.drop(['content', 'Movement'],1)\"\"\"\n",
    "print(\"Class Balance: {}\".format(sorted(Counter(labels).items())))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(earnings_tfidf, labels,\n",
    "                                                    test_size=sample_split,\n",
    "                                                    random_state=0)\n",
    "\n",
    "\n",
    "X_train_res, y_train_res = ros.fit_sample(X_train, y_train)\n",
    "\n",
    "print(\"Class Balance: {}\".format(sorted(Counter(y_train_res).items())))\n",
    "print(\"Class Balance: {}\".format(sorted(Counter(y_test).items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.0\n",
      "\n",
      "Test set score: 0.53125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.10      0.15        31\n",
      "           1       0.55      0.90      0.69        69\n",
      "           2       0.43      0.11      0.17        28\n",
      "\n",
      "    accuracy                           0.53       128\n",
      "   macro avg       0.44      0.37      0.34       128\n",
      "weighted avg       0.47      0.53      0.44       128\n",
      "\n",
      "[[ 3 26  2]\n",
      " [ 5 62  2]\n",
      " [ 1 24  3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics \n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train_res, y_train_res))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jkovach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6286919831223629\n",
      "\n",
      "Test set score: 0.46875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.35      0.33        31\n",
      "           1       0.65      0.54      0.59        69\n",
      "           2       0.34      0.43      0.38        28\n",
      "\n",
      "    accuracy                           0.47       128\n",
      "   macro avg       0.43      0.44      0.43       128\n",
      "weighted avg       0.50      0.47      0.48       128\n",
      "\n",
      "[[11 11  9]\n",
      " [18 37 14]\n",
      " [ 7  9 12]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.8)\n",
    "train = lr.fit(X_train_res, y_train_res)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('Training set score:', lr.score(X_train_res, y_train_res))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.0\n",
      "\n",
      "Test set score: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.10      0.14        31\n",
      "           1       0.58      0.83      0.68        69\n",
      "           2       0.53      0.32      0.40        28\n",
      "\n",
      "    accuracy                           0.54       128\n",
      "   macro avg       0.45      0.41      0.41       128\n",
      "weighted avg       0.49      0.54      0.49       128\n",
      "\n",
      "[[ 3 24  4]\n",
      " [ 8 57  4]\n",
      " [ 2 17  9]]\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 1000,\n",
    "          'max_depth': 5,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "train = clf.fit(X_train_res, y_train_res)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Training set score:', clf.score(X_train_res, y_train_res))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9430379746835443\n",
      "\n",
      "Test set score: 0.4765625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.32      0.34        31\n",
      "           1       0.58      0.61      0.60        69\n",
      "           2       0.32      0.32      0.32        28\n",
      "\n",
      "    accuracy                           0.48       128\n",
      "   macro avg       0.42      0.42      0.42       128\n",
      "weighted avg       0.47      0.48      0.47       128\n",
      "\n",
      "[[10 15  6]\n",
      " [14 42 13]\n",
      " [ 4 15  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate SVM object\n",
    "svm = SVC(kernel = 'linear')\n",
    "train = svm.fit(X_train_res, y_train_res)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print('Training set score:', svm.score(X_train_res, y_train_res))\n",
    "print('\\nTest set score:', svm.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\"\"\"nlp = spacy.load('en')\n",
    "\n",
    "corpus = '\\n'.join(list(earnings_df['content']))\n",
    "\n",
    "def word_frequencies(text):\n",
    "    return Counter(text.split())\n",
    "    \n",
    "# The most frequent words:\n",
    "freq = word_frequencies(corpus).most_common(100)\n",
    "common_words = [word[0] for word in freq]\n",
    "\n",
    "def remove_common(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in common_words])\n",
    "    return text\n",
    "\n",
    "earnings_df['redux'] = earnings_df['content'].apply(remove_common)\n",
    "earnings_df.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
